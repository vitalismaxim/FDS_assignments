{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2735c935",
   "metadata": {
    "id": "2735c935"
   },
   "source": [
    "<h1><center> Assignment 1: EDA United Nations General Debate Corpus  </center></h1>\n",
    "\n",
    "We are now going to give the first steps into exploring the United Nations General Debate Corpus. <span style=\"color:red\">This dataset will be used in Group Assignment I (due date, Monday 3 October, 23:59)</span>. It is expected that you will pose a questions about the dataset, explore it, and combine it with other datasets (e.g., the Happiness Report 2022 that we've been using, or the International Trade Dataset). \n",
    "\n",
    "We will use the *the UN General Debate Corpus (UNGDC)*, which introduces the corpus of texts of UN General Debate statements from 1970 (Session 25) to 2020 (Session 75). More info [here](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/0TJX8Y). Make sure to download the file <code>UNGDC_1970-2020.tar.gz</code> and extract the folder <code>TXT/</code> to the same directory as the current Jupyter notebook. You are also advised to collect and pre-process more recent speeches data (e.g. [session 76](https://gadebate.un.org/generaldebate76/en/))\n",
    "\n",
    "Notice that the 77th session of the UN General Assembly - where the 2022 debates will happen - will occur in 14-30 September 2021, precisely during the time you'll be working in Assignment 1. More info [here](https://www.un.org/en/ga/77/meetings/). \n",
    "\n",
    "You might find useful to have a dataset with the full name and 3-code description of countries. You can find that data [here](https://unstats.un.org/unsd/methodology/m49/overview/). Download the correspondig CSV file (named 'UNSD — Methodology.csv') and place it into the same folder as this notebook.\n",
    "\n",
    "We will start by loading the speeches text to a table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0b3ce3",
   "metadata": {
    "id": "cd0b3ce3",
    "outputId": "2389f396-4af3-408f-ead4-16e595e79676"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sessions = np.arange(25, 76)\n",
    "data=[]\n",
    "\n",
    "for session in sessions:\n",
    "    directory = \"./TXT/Session \"+str(session)+\" - \"+str(1945+session)\n",
    "    for filename in os.listdir(directory):\n",
    "        f = open(os.path.join(directory, filename), encoding=\"utf8\")\n",
    "        if filename[0]==\".\": #ignore hidden files\n",
    "            continue\n",
    "        splt = filename.split(\"_\")\n",
    "        data.append([session, 1945+session, splt[0], f.read()])\n",
    "\n",
    "        \n",
    "df_speech = pd.DataFrame(data, columns=['Session','Year','ISO-alpha3 Code','Speech'])\n",
    "\n",
    "df_speech.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072f332c",
   "metadata": {
    "id": "072f332c"
   },
   "source": [
    "Download the 'UNSD — Methodology.csv' ([link](https://unstats.un.org/unsd/methodology/m49/overview/)) file and and try to load it. Call the resulting dataframe 'df_codes'. \n",
    "Please check what is the separator used. Why is that separator used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e54e04",
   "metadata": {
    "id": "60e54e04",
    "outputId": "2dda9a07-4c8b-4e52-e8a0-744f58857013"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcfe8b4d",
   "metadata": {
    "id": "bcfe8b4d"
   },
   "source": [
    "**Q1: Can you create a merged DataFrame — merge between df_codes and df_speech according to ISO-alpha3 and composed of columns \\[\"Country or Area\", \"Region Name\",\"Sub-region Name\", \"ISO-alpha3 Code\",\"Least Developed Countries (LDC)\", \"Session\", \"Year\", \"Speech\"\\]? It would be convinient to have index as (Year, 'ISO-alpha3 Code')**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde2a21b",
   "metadata": {
    "id": "cde2a21b",
    "outputId": "00428c18-fc02-4953-f6d3-d1921abc52d4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbc0358d",
   "metadata": {
    "id": "fbc0358d"
   },
   "source": [
    "We are now going to use NLTK\n",
    "\n",
    "Please run the cell below to import NLTK and download the needed resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49d6e45",
   "metadata": {
    "id": "d49d6e45",
    "outputId": "77120fca-6e61-45a6-c938-dc6bbe16bfb9"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822564c9",
   "metadata": {
    "id": "822564c9"
   },
   "source": [
    "Let us now see some examples of word analysis with NLTK:\n",
    "\n",
    "Which were the most frequent words used in the Austrian Speech in 1970?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10182ad0",
   "metadata": {
    "id": "10182ad0",
    "outputId": "d808c3c9-d99d-47f4-cd9b-cadae1d06d7a"
   },
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# load text of Austria in 1970\n",
    "text = df_un_merged.loc[1970,'AUT'][\"Speech\"]\n",
    "\n",
    "# tokenize words\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# compute word frequency\n",
    "freq = FreqDist(words)\n",
    "\n",
    "# show 30 most frequent words\n",
    "freq.most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc239cad",
   "metadata": {},
   "source": [
    "**Q2: Plot the histogram with the top most used words:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6593e2",
   "metadata": {
    "id": "4a6593e2",
    "outputId": "9acfffe4-4b57-42a2-ee4f-5cd9a278134e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ab49af4",
   "metadata": {
    "id": "1ab49af4"
   },
   "source": [
    "Notice that the most frequent words are not that informative about the Austrian speech (the, of, to...). These words are often called *stop-words*. These words are generally filtered out before processing text (natural language). These are actually some of the most common words in any language (articles, prepositions, pronouns, conjunctions, etc) but do not add much information to the text. Let's now use NLTK to filter those words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93adb656",
   "metadata": {
    "id": "93adb656",
    "outputId": "2a61f41a-7ce0-43d8-ca05-f3c3475459dc"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preprocess(words):\n",
    "    sw = stopwords.words(\"english\")\n",
    "    no_sw = []\n",
    "    for w in words:\n",
    "        if (w not in sw):\n",
    "            no_sw.append(w)\n",
    "    return no_sw\n",
    "\n",
    "text = df_un_merged.loc[2002,\"AFG\"][\"Speech\"]\n",
    "\n",
    "words = word_tokenize(text)\n",
    "words = preprocess(words)\n",
    "freq = FreqDist(words)\n",
    "\n",
    "freq.plot(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5081d6eb",
   "metadata": {
    "id": "5081d6eb"
   },
   "source": [
    "**Q3: Can you change the method preprocess to put all words in lower case, remove punctuation and remove non-informative words (e.g., United Nations)?**\n",
    "\n",
    "Tip: the method isalpha() might be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc88891e",
   "metadata": {
    "id": "cc88891e",
    "outputId": "9f4f822f-b5ea-4570-8194-c1b7abbd2b15"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44ba5961",
   "metadata": {
    "id": "44ba5961"
   },
   "source": [
    "A regular expression is a sequence of characters that specifies a pattern. Usually, such patterns are used by to find, match, replace sub-strings within a document. Regular expressions have a particular syntax and are often useful to clean and pre-process textual data. Here one example where the regular expression 'afg.\\*' is used to match any word that starts with afg and is followed by any character. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cd5015",
   "metadata": {
    "id": "73cd5015",
    "outputId": "9b44ad57-28b1-4fd2-caac-09fe2c0c9c98"
   },
   "outputs": [],
   "source": [
    "# Regular expression example\n",
    "s = set({})\n",
    "import re\n",
    "for w in words:\n",
    "    if re.match('afg.*n$', w):\n",
    "        s.add(w)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1cdfdc",
   "metadata": {
    "id": "bf1cdfdc"
   },
   "source": [
    "Another useful usage of NLTK is performing sentiment analysis.\n",
    "\n",
    "Sentiment analysis can be seen as the process of automatically classifying text into positive or negative sentiment categories. With NLTK, you can employ these algorithms without effort. This was also called opinion mining.\n",
    "\n",
    "In the political field, sentiment analysis is used to keep track of political view, to detect consistency and inconsistency between statements and actions at the government level or to derive the opinion or attitude of a speaker.\n",
    "\n",
    "NLTK implements VADER (Valence Aware Dictionary and sEntiment Reasoner), which is a lexicon and rule-based sentiment analysis. VADER uses a list of lexical features (e.g., words) which are generally labeled according to their semantic orientation as either positive or negative. VADER not only tells about the Positivity and Negativity score but also tells us about how positive or negative a sentiment is.\n",
    "\n",
    "NLTK implements VADER through the module SentimentIntensityAnalyzer. Below an example of application (with natural limitations as VADER is specifically attuned to sentiments expressed in **social media**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293744e6",
   "metadata": {
    "id": "293744e6",
    "outputId": "e204f46d-02aa-46b7-9aa9-59f0e6195aca"
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "vecUSA = [sia.polarity_scores(df_un_merged.loc(axis=0)[i,\"USA\"][\"Speech\"])['pos'] for i in np.arange(1971, 2021)]\n",
    "vecRUS = [sia.polarity_scores(df_un_merged.loc(axis=0)[i,\"RUS\"][\"Speech\"])['pos'] for i in np.arange(1971, 2021)]\n",
    "vecCHN = [sia.polarity_scores(df_un_merged.loc(axis=0)[i,\"CHN\"][\"Speech\"])['pos'] for i in np.arange(1971, 2021)]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.arange(1971, 2021), vecUSA)\n",
    "ax.plot(np.arange(1971, 2021), vecRUS)\n",
    "ax.plot(np.arange(1971, 2021), vecCHN)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73ebf83",
   "metadata": {
    "id": "f73ebf83"
   },
   "source": [
    "Notice that depending on how you're going to use the UN Debates corpus, you might need to perform some data cleaning. For example, try to execute the following code:\n",
    "\n",
    "<code>df_un_merged.loc[1970,\"YUG\"]</code> (too many NaN)\n",
    "\n",
    "<code>df_un_merged.loc[2020,\"POR\"]</code> (too many NaN)\n",
    "\n",
    "<code>df_un_merged.loc[2020,\"PRT\"]</code> (error)\n",
    "\n",
    "**Q4: Can you identify and solve the problems?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55027f31",
   "metadata": {
    "id": "55027f31",
    "outputId": "2ba1045a-f727-4a6a-dbfd-89e8c9bab4f4"
   },
   "outputs": [],
   "source": [
    "df_un_merged.loc[2020,\"POR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934946b5",
   "metadata": {
    "id": "934946b5",
    "outputId": "1d164397-0973-4ccf-ddf1-e62756706899"
   },
   "outputs": [],
   "source": [
    "df_un_merged.loc[2019,\"POR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97281d34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab3-Assignment1-Aux.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
